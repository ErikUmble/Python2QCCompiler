{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import joinedload\n",
    "\n",
    "from benchmarklib import BenchmarkDatabase\n",
    "from rbf import RandomBooleanFunctionTrial, RandomBooleanFunction\n",
    "from benchmarklib.compilers import CompileType, XAGCompiler\n",
    "\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, RuntimeJobNotFound\n",
    "\n",
    "import logging\n",
    "from typing import Iterable, List, Tuple, Dict, Any, Union, Optional\n",
    "import qiskit\n",
    "from qiskit.providers import Backend\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "import random\n",
    "\n",
    "from sqlalchemy import select, func\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from benchmarklib import BenchmarkDatabase\n",
    "from benchmarklib import BatchQueue\n",
    "from benchmarklib.compilers import SynthesisCompiler\n",
    "from benchmarklib.core.database import BackendPropertyManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_TOKEN_OLD = os.getenv(\"API_TOKEN_OLD\")\n",
    "API_INSTANCE_OLD = os.getenv(\"API_INSTANCE_OLD\")\n",
    "service = QiskitRuntimeService()  # default service with new credentials\n",
    "service_old = QiskitRuntimeService(\n",
    "    channel='ibm_quantum_platform',\n",
    "    token=API_TOKEN_OLD,\n",
    "    instance=API_INSTANCE_OLD\n",
    ")\n",
    "backend = service.backend(\"ibm_rensselaer\")\n",
    "benchmark_db = BenchmarkDatabase(\"rbf.db\", RandomBooleanFunction, RandomBooleanFunctionTrial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f002a",
   "metadata": {},
   "source": [
    "### Backend Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af01307",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_db = BackendPropertyManager(\"rbf.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = backend.properties()\n",
    "error_rates = {}\n",
    "for g in backend.configuration().basis_gates:\n",
    "    error_rates[g] = []\n",
    "    for gate_info in properties.gates:\n",
    "        d = gate_info.to_dict()\n",
    "        if d[\"gate\"] == g:\n",
    "            for param in d[\"parameters\"]:\n",
    "                if param[\"name\"] == \"gate_error\":\n",
    "                    error_rates[g].append(param[\"value\"])\n",
    "print(\"Current backend gate error rates:\")             \n",
    "for g in error_rates:\n",
    "    if len(error_rates[g]) > 0:\n",
    "        print(f\"{g}: avg error = {np.mean(error_rates[g]):.2e} (min={np.min(error_rates[g]):.2e}, max={np.max(error_rates[g]):.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1714994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def get_mean_gate_errors(at: datetime.date):\n",
    "    if at in cache:\n",
    "        return cache[at]\n",
    "    props = backend_db.latest(backend, at)\n",
    "    cache[at] = props.get_average_gate_errors()\n",
    "    return cache[at]\n",
    "\n",
    "med_cache = {}\n",
    "def get_median_gate_errors(at: datetime.date):\n",
    "    if at in med_cache:\n",
    "        return med_cache[at]\n",
    "    props = backend_db.latest(backend, at)\n",
    "    med_cache[at] = {g : np.median(v) for g, v in props.get_gate_errors().items()}\n",
    "    return med_cache[at]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b908ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark_db.session() as session:\n",
    "    query = (select(RandomBooleanFunction.num_vars, RandomBooleanFunction.complexity)\n",
    "            .select_from(RandomBooleanFunctionTrial).join(RandomBooleanFunctionTrial.problem)\n",
    "            .where(RandomBooleanFunctionTrial._circuit_qpy != None).limit(20000)\n",
    "            )\n",
    "    trials = session.execute(query).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for num_vars, complexity in trials:\n",
    "    counts[(num_vars, complexity)] = counts.get((num_vars, complexity), 0) + 1\n",
    "\n",
    "# create matrix\n",
    "max_vars = max(k[0] for k in counts.keys())\n",
    "max_complexity = max(k[1] for k in counts.keys())\n",
    "matrix = np.zeros((max_vars, max_complexity), dtype=int)\n",
    "for (num_vars, complexity), count in counts.items():\n",
    "    matrix[num_vars - 1, complexity - 1] = count\n",
    "plt.imshow(matrix, cmap='Blues', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72253ec",
   "metadata": {},
   "source": [
    "### Analytic Success Rate Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_gate_error1(circuit, gate_errors):\n",
    "    assert circuit is not None\n",
    "    expected_success_rate = 1.0\n",
    "    for inst in circuit.data:\n",
    "        expected_success_rate *= (1 - gate_errors.get(inst[0].name, 0.0))\n",
    "    return expected_success_rate\n",
    "\n",
    "def compute_analytic_success_rate_estimate1(circuit, created_at=None):\n",
    "    created_at_date = created_at.date() if created_at is not None else datetime.date.today()\n",
    "    analytic_estimate1 = compute_accumulated_gate_error1(circuit, gate_errors = get_median_gate_errors(created_at_date))\n",
    "    analytic_estimate2 = compute_accumulated_gate_error1(circuit, gate_errors = get_mean_gate_errors(created_at_date))\n",
    "    return (analytic_estimate1 + analytic_estimate2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = benchmark_db.query(\n",
    "    select(RandomBooleanFunctionTrial)\n",
    "        .where(\n",
    "            RandomBooleanFunctionTrial._circuit_qpy != None,\n",
    "            RandomBooleanFunctionTrial.counts != None\n",
    "        )\n",
    "        .options(joinedload(RandomBooleanFunctionTrial.problem))\n",
    "        .order_by(func.random())\n",
    "        .limit(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "for trial in trials:\n",
    "    analytic_estimate = compute_analytic_success_rate_estimate1(trial.circuit, trial.created_at)\n",
    "    y = 1 if trial.calculate_success_rate() >= 0.5 else 0\n",
    "    y_pred = 1 if analytic_estimate >= 0.5 else 0\n",
    "    confusion_matrix[y, y_pred] += 1\n",
    "    \n",
    "\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "accuracy = (tp + tn) / np.sum(confusion_matrix)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d03e61",
   "metadata": {},
   "source": [
    "### Create Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e60afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Comment this Exception out if you actually want to re-create the train/test split\")\n",
    "# designate training and test sets\n",
    "num_train = 1500\n",
    "num_test = 500\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "for i, trial in enumerate(benchmark_db.query(\n",
    "    select(RandomBooleanFunctionTrial)\n",
    "    .where(RandomBooleanFunctionTrial._circuit_qpy != None)\n",
    "    .order_by(func.random())\n",
    "    .limit(num_train + num_test)\n",
    "    .options(joinedload(RandomBooleanFunctionTrial.problem))\n",
    ")):\n",
    "    if i < num_train:\n",
    "        train_ids.append(trial.id)\n",
    "    else:\n",
    "        test_ids.append(trial.id)\n",
    "        \n",
    "# save datasets to disk for reproducibility\n",
    "with open(\"train_ids.json\", \"w\") as f:\n",
    "    json.dump(train_ids, f)\n",
    "with open(\"test_ids.json\", \"w\") as f:\n",
    "    json.dump(test_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "def get_features(trial: RandomBooleanFunctionTrial) -> Tuple[np.array, int]:\n",
    "    features = np.ones(7)\n",
    "    features[1] = trial.circuit_num_qubits\n",
    "    features[2] = trial.circuit_depth\n",
    "    features[3] = trial.circuit_op_counts.get('ecr', 0)\n",
    "    features[4] = trial.circuit_op_counts.get('rz', 0)\n",
    "    features[5] = trial.circuit_op_counts.get('sx', 0)\n",
    "    features[6] = trial.circuit_op_counts.get('x', 0)\n",
    "    label = 1 if trial.calculate_success_rate() > 0.5 else 0\n",
    "    return features, label\n",
    "\n",
    "def get_features2(trial: RandomBooleanFunctionTrial) -> Tuple[np.array, int]:\n",
    "    created_at = trial.created_at\n",
    "    gate_errors = get_mean_gate_errors(created_at.date())\n",
    "\n",
    "    features = np.ones(7)\n",
    "    features[1] = trial.circuit_num_qubits\n",
    "    features[2] = trial.circuit_depth\n",
    "    features[3] = trial.circuit_op_counts.get('ecr', 0) * gate_errors.get('ecr', 0)\n",
    "    features[4] = trial.circuit_op_counts.get('rz', 0) * gate_errors.get('rz', 0)\n",
    "    features[5] = trial.circuit_op_counts.get('sx', 0) * gate_errors.get('sx', 0)\n",
    "    features[6] = trial.circuit_op_counts.get('x', 0) * gate_errors.get('x', 0)\n",
    "    label = 1 if trial.calculate_success_rate() > 0.5 else 0\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def get_trials_by_ids(db: BenchmarkDatabase, ids: List[int]) -> List[RandomBooleanFunctionTrial]:\n",
    "    return db.query(\n",
    "        select(RandomBooleanFunctionTrial)\n",
    "        .where(RandomBooleanFunctionTrial.id.in_(ids))\n",
    "        .options(joinedload(RandomBooleanFunctionTrial.problem))\n",
    "    )\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "with open(\"train_ids.json\", \"r\") as f:\n",
    "    train_ids = json.load(f)\n",
    "with open(\"test_ids.json\", \"r\") as f:\n",
    "    test_ids = json.load(f)\n",
    "\n",
    "# construct train data set\n",
    "train_trials = get_trials_by_ids(benchmark_db, train_ids)\n",
    "train_data = [get_features(trial) for trial in train_trials]\n",
    "X = np.zeros((len(train_trials), len(train_data[0][0])))\n",
    "Y = np.zeros((len(train_trials), 1))\n",
    "for i, data in enumerate(train_data):\n",
    "    X[i] = data[0]\n",
    "    Y[i, 0] = data[1]\n",
    "\n",
    "# construct test data set\n",
    "test_trials = get_trials_by_ids(benchmark_db, test_ids)\n",
    "test_data = [get_features(trial) for trial in test_trials]\n",
    "X_test = np.zeros((len(test_trials), len(test_data[0][0])))\n",
    "Y_test = np.zeros((len(test_trials), 1))\n",
    "for i, data in enumerate(test_data):\n",
    "    X_test[i] = data[0]\n",
    "    Y_test[i, 0] = data[1]\n",
    "\n",
    "# cache datasets to disk\n",
    "np.save(\"train_features.npy\", X)\n",
    "np.save(\"train_labels.npy\", Y)\n",
    "np.save(\"test_features.npy\", X_test)\n",
    "np.save(\"test_labels.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494d8bc",
   "metadata": {},
   "source": [
    "### Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gd(X_aug, y, learning_rate=0.01, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Logistic regression using regular gradient descent\n",
    "    \"\"\"\n",
    "    N, d = X_aug.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        update_vec = np.zeros(d)\n",
    "        for i in range(N):\n",
    "            power = -y[i] * (w.T @ X_aug[i, :])\n",
    "            update_vec += -y[i] * X_aug[i, :] * (np.exp(power) / (1 + np.exp(power)))\n",
    "        w -= learning_rate * (update_vec / N)\n",
    "        errors.append(np.mean(np.log(1+np.exp(-y * (X_aug @ w)))))\n",
    "\n",
    "    plt.plot(range(len(errors)), errors)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    return w\n",
    "\n",
    "def logistic_regression_sgd(X_aug, y, learning_rate=0.01, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Logistic regression using stochastic gradient descent\n",
    "    constraint: all weights must be negative besides the bias term (since each should impact the circuit success negatively)\n",
    "    \"\"\"\n",
    "    # if y is 0/1, convert to -1/1\n",
    "    y = np.where(y <= 0, -1, 1)\n",
    "\n",
    "    N, d = X_aug.shape\n",
    "    w = np.zeros(d)\n",
    "    #errors = []\n",
    "    for _ in range(max_iter):\n",
    "        i = np.random.randint(0, N)\n",
    "        power = -y[i] * (w.T @ X_aug[i, :])\n",
    "        grad = -y[i] * X_aug[i, :] * (np.exp(power) / (1 + np.exp(power)))\n",
    "        w -= learning_rate * grad\n",
    "        w[1:] = np.minimum(w[1:], 0)  # weight constraints\n",
    "        #errors.append(np.mean(np.log(1+np.exp(-y * (X_aug @ w)))))\n",
    "    #plt.plot(range(len(errors)), errors)\n",
    "    #plt.show()\n",
    "    #plt.clf()\n",
    "    return w\n",
    "\n",
    "\n",
    "def logistic_regression_neg_gd(X, y, lr=0.01, epochs=1000):\n",
    "    y_pm = 2*y - 1\n",
    "    N, d = X.shape\n",
    "    v = np.zeros(d)  # unconstrained\n",
    "    b = 0.0\n",
    "    for _ in range(epochs):\n",
    "        grad_b_total = 0\n",
    "        grad_v_total = np.zeros(d)\n",
    "        for i in range(N):\n",
    "            i = np.random.randint(0, N)\n",
    "            w = -np.log1p(np.exp(v))  # -softplus(v)\n",
    "            z = w @ X[i, :] + b\n",
    "            s = 1 / (1 + np.exp(y_pm[i]*z))\n",
    "            grad_z = -y_pm[i] * s\n",
    "            grad_v = grad_z * X[i, :] * (-1/(1+np.exp(-v)))  # derivative of -softplus\n",
    "            grad_b = grad_z\n",
    "            grad_v_total += grad_v\n",
    "            grad_b_total += grad_b\n",
    "\n",
    "        v -= (grad_v_total / N) * lr\n",
    "        b -= (grad_b_total / N) * lr\n",
    "    w = -np.log1p(np.exp(v))\n",
    "    w[0] += b[0]  # incorporate bias into weight vector\n",
    "    return w\n",
    "\n",
    "def logistic_regression_constrained_sgd(X_unc, X_neg, Y, lr=0.01, epochs=1000, lam=0.01):\n",
    "    \"\"\"\n",
    "    Logistic regression with constraint that certain weights be negative.\n",
    "    For features X, split into X_unc (unconstrained weights) and X_neg (negative weights) so that the prediction \n",
    "    is Y_hat = X_unc @ w_unc + X_neg @ w_neg + b\n",
    "    where w_neg is all negative (via -softplus), and b is a bias term\n",
    "    \"\"\"\n",
    "    Y = 2*Y - 1  # convert y 0/1 to -1/+1\n",
    "    N, d_unc = X_unc.shape\n",
    "    N, d_neg = X_neg.shape\n",
    "    w_unc = np.zeros(d_unc)\n",
    "    v_neg = np.zeros(d_neg)  # w_neg = -softplus(v_neg)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        i = np.random.randint(0, N)\n",
    "\n",
    "        # softplus\n",
    "        w_neg = -np.log1p(np.exp(v_neg))\n",
    "        z = X_unc[i, :] @ w_unc + X_neg[i, :] @ w_neg + b\n",
    "\n",
    "        # logistic loss grad\n",
    "        grad_z = -Y[i] / (1 + np.exp(Y[i] * z))\n",
    "\n",
    "        # grad wrt w_unc\n",
    "        grad_unc = grad_z * X_unc[i]\n",
    "        grad_unc += lam * w_unc  # L2 regularization\n",
    "\n",
    "        # grad wrt v_neg\n",
    "        sigmoid_v = 1/ (1 + np.exp(-v_neg))\n",
    "        grad_neg = grad_z * X_neg[i] * (-sigmoid_v)\n",
    "        grad_neg += lam * w_neg * (-sigmoid_v) # L2 regularization\n",
    "\n",
    "        # grad wrt bias\n",
    "        grad_b = grad_z\n",
    "\n",
    "        # update\n",
    "        w_unc -= lr * grad_unc\n",
    "        v_neg -= lr * grad_neg\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    w_neg = -np.log1p(np.exp(v_neg))\n",
    "    return w_unc, w_neg, b\n",
    "\n",
    "def logistic_regression_constrained_gd(X_unc, X_neg, Y, lr=0.01, epochs=1000, lam=0.01):\n",
    "    \"\"\"\n",
    "    Logistic regression with constraint that certain weights be negative.\n",
    "    For features X, split into X_unc (unconstrained weights) and X_neg (negative weights) so that the prediction \n",
    "    is Y_hat = X_unc @ w_unc + X_neg @ w_neg + b\n",
    "    where w_neg is all negative (via -softplus), and b is a bias term\n",
    "    \"\"\"\n",
    "    Y = 2*Y - 1  # convert y 0/1 to -1/+1\n",
    "    N, d_unc = X_unc.shape\n",
    "    N, d_neg = X_neg.shape\n",
    "    w_unc = np.zeros(d_unc)\n",
    "    v_neg = np.zeros(d_neg)  # w_neg = -softplus(v_neg)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        grad_unc_total = np.zeros(d_unc)\n",
    "        grad_neg_total = np.zeros(d_neg)\n",
    "        grad_b_total = 0.0\n",
    "\n",
    "        for i in range(N):\n",
    "            # softplus\n",
    "            w_neg = -np.log1p(np.exp(v_neg))\n",
    "            z = X_unc[i, :] @ w_unc + X_neg[i, :] @ w_neg + b\n",
    "\n",
    "            # logistic loss grad\n",
    "            grad_z = -Y[i] / (1 + np.exp(Y[i] * z))\n",
    "\n",
    "            # grad wrt w_unc\n",
    "            grad_unc = grad_z * X_unc[i]\n",
    "            grad_unc += lam * w_unc  # L2 regularization\n",
    "            grad_unc_total += grad_unc\n",
    "\n",
    "            # grad wrt v_neg\n",
    "            sigmoid_v = 1/ (1 + np.exp(-v_neg))\n",
    "            grad_neg = grad_z * X_neg[i] * (-sigmoid_v)\n",
    "            grad_neg += lam * w_neg * (-sigmoid_v) # L2 regularization\n",
    "            grad_neg_total += grad_neg\n",
    "\n",
    "            # grad wrt bias\n",
    "            grad_b = grad_z\n",
    "            grad_b_total += grad_b\n",
    "\n",
    "        # update\n",
    "        w_unc -= lr * (grad_unc_total / N)\n",
    "        v_neg -= lr * (grad_neg_total / N)\n",
    "        b -= lr * (grad_b_total / N)\n",
    "\n",
    "    w_neg = -np.log1p(np.exp(v_neg))\n",
    "    return w_unc, w_neg, b\n",
    "\n",
    "\n",
    "def logistic_regression_with_freeze(X_unc, X_freeze, Y, w_freeze, lr=0.01, epochs=1000, lam=0.01):\n",
    "    \"\"\"\n",
    "    Logistic regression with constraint that certain weights be fixed (not learned).\n",
    "    For features X, split into X_unc (unconstrained weights) and X_freeze (frozen weights) so that the prediction \n",
    "    is Y_hat = X_unc @ w_unc + X_freeze @ w_freeze + b\n",
    "    where b is a bias term\n",
    "    \"\"\"\n",
    "    Y = 2*Y - 1  # convert y 0/1 to -1/+1\n",
    "    N, d_unc = X_unc.shape\n",
    "    N, d_freeze = X_freeze.shape\n",
    "    w_unc = np.zeros(d_unc)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        i = np.random.randint(0, N)\n",
    "\n",
    "        z = X_unc[i, :] @ w_unc + X_freeze[i, :] @ w_freeze + b\n",
    "\n",
    "        # logistic loss grad\n",
    "        grad_z = -Y[i] / (1 + np.exp(Y[i] * z))\n",
    "\n",
    "        # grad wrt w_unc\n",
    "        grad_unc = grad_z * X_unc[i]\n",
    "        grad_unc += lam * w_unc  # L2 regularization\n",
    "\n",
    "        # grad wrt bias\n",
    "        grad_b = grad_z\n",
    "\n",
    "        # update\n",
    "        w_unc -= lr * grad_unc\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    return w_unc, b\n",
    "        \n",
    "\n",
    "def logistic_regression_neg(X, y, lr=0.01, epochs=1000, lam=0.01):\n",
    "    \"\"\"\n",
    "    Logistic regression with constraint: all weights <= 0\n",
    "    using reparameterization w = -softplus(v)\n",
    "    with L2 regularization strength 'lam'\n",
    "    \"\"\"\n",
    "    y_pm = 2*y - 1\n",
    "    N, d = X.shape\n",
    "    v = np.zeros(d)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        i = np.random.randint(0, N)\n",
    "        # reparameterize\n",
    "        w = -np.log1p(np.exp(v))  # = -softplus(v)\n",
    "        z = w @ X[i, :] + b\n",
    "\n",
    "        # logistic loss grad\n",
    "        s = 1 / (1 + np.exp(y_pm[i] * z))\n",
    "        grad_z = -y_pm[i] * s\n",
    "\n",
    "        # gradient wrt v (chain rule)\n",
    "        sigmoid_v = 1 / (1 + np.exp(-v))\n",
    "        grad_v = grad_z * X[i, :] * (-sigmoid_v)\n",
    "        \n",
    "        grad_v += -lam * w * sigmoid_v  # regularization\n",
    "\n",
    "        grad_b = grad_z\n",
    "\n",
    "        # update\n",
    "        v -= lr * grad_v\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    # final weights\n",
    "    w = -np.log1p(np.exp(v))\n",
    "    w[0] += b[0]  # incorporate bias into weight vector\n",
    "    return w\n",
    "\n",
    "def logistic_regression_neg_gd(X, y, lr=0.01, epochs=1000, lam=0.01):\n",
    "    \"\"\"\n",
    "    Logistic regression with constraint: all weights <= 0\n",
    "    using reparameterization w = -softplus(v)\n",
    "    with L2 regularization strength 'lam'\n",
    "    \"\"\"\n",
    "    y_pm = 2*y - 1\n",
    "    N, d = X.shape\n",
    "    v = np.zeros(d)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        grad_v_total = np.zeros(d)\n",
    "        grad_b_total = 0.0\n",
    "\n",
    "        for i in range(N):\n",
    "            # reparameterize\n",
    "            w = -np.log1p(np.exp(v))  # = -softplus(v)\n",
    "            z = w @ X[i, :] + b\n",
    "\n",
    "            # logistic loss grad\n",
    "            s = 1 / (1 + np.exp(y_pm[i] * z))\n",
    "            grad_z = -y_pm[i] * s\n",
    "\n",
    "            sigmoid_v = 1 / (1 + np.exp(-v))\n",
    "            grad_v = grad_z * X[i, :] * (-sigmoid_v)\n",
    "\n",
    "            grad_v += -lam * w * sigmoid_v  # regularization\n",
    "\n",
    "            grad_b = grad_z\n",
    "\n",
    "            grad_v_total += grad_v\n",
    "            grad_b_total += grad_b\n",
    "\n",
    "        # update\n",
    "        v -= lr * (grad_v_total / N)\n",
    "        b -= lr * (grad_b_total / N)\n",
    "\n",
    "    # final weights\n",
    "    w = -np.log1p(np.exp(v))\n",
    "    w[0] += b[0]  # incorporate bias into weight vector\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb29ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "\n",
    "def evaluate(X, Y, w):\n",
    "    Y_pred = sigmoid(X @ w)\n",
    "    confusion_matrix_in = confusion_matrix(np.round(Y), np.round(Y_pred))\n",
    "    cross_entropy_in = log_loss(np.round(Y), Y_pred) / len(Y)\n",
    "    print(\"In-sample Confusion Matrix:\")\n",
    "    print(confusion_matrix_in)\n",
    "    print(f\"(Cross Entropy): {cross_entropy_in}\")\n",
    "    percent_correct_in = np.trace(confusion_matrix_in) / np.sum(confusion_matrix_in)\n",
    "    print(f\"Percent Correct: {percent_correct_in * 100:.2f}%\")\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfca4de",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29571987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = np.load(\"train_features.npy\")\n",
    "Y = np.load(\"train_labels.npy\")\n",
    "X_test = np.load(\"test_features.npy\")\n",
    "Y_test = np.load(\"test_labels.npy\")\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f068f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute VIF measures\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_data = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba100414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use partial negative constrained regression\n",
    "w_unc, w_neg, b = logistic_regression_constrained_gd(X[:, 1:3], X[:, 3:], Y, lr=0.01, epochs=1000, lam=1)\n",
    "w = np.concatenate((b, w_unc, w_neg))\n",
    "print(w)\n",
    "print(\"In-sample Evaluation:\")\n",
    "evaluate(X, Y, w)\n",
    "\n",
    "print(\"Out-of-sample Evaluation:\")\n",
    "evaluate(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4740676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full negative constrained regression\n",
    "w = logistic_regression_neg(X, Y, lr=0.01, epochs=10000, lam=0.1)\n",
    "print(w)\n",
    "print(\"In-sample Evaluation:\")\n",
    "evaluate(X, Y, w)\n",
    "\n",
    "print(\"Out-of-sample Evaluation:\")\n",
    "evaluate(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use frozen weights based on known backend parameters\n",
    "avg_error = get_mean_gate_errors(datetime.date.today())\n",
    "w_freeze = np.array([-avg_error.get(g, 0) for g in [\"ecr\", \"rz\", \"sx\", \"x\"]])\n",
    "w_unc, b = logistic_regression_with_freeze(X[:, 1:3], X[:, 3:], Y, w_freeze, lr=0.002, epochs=400000, lam=2)\n",
    "w = np.concatenate((b, w_unc, w_freeze))\n",
    "print(w)\n",
    "print(\"In-sample Evaluation:\")\n",
    "evaluate(X, Y, w)\n",
    "\n",
    "print(\"Out-of-sample Evaluation:\")\n",
    "evaluate(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use frozen weights based on known backend parameters\n",
    "avg_error = get_median_gate_errors(datetime.date.today())\n",
    "w_freeze = np.array([-avg_error.get(g, 0) for g in [\"ecr\", \"rz\", \"sx\", \"x\"]])\n",
    "w_unc, b = logistic_regression_with_freeze(X[:, 1:3], X[:, 3:], Y, w_freeze, lr=0.002, epochs=400000, lam=2)\n",
    "w = np.concatenate((b, w_unc, w_freeze))\n",
    "print(w)\n",
    "print(\"In-sample Evaluation:\")\n",
    "evaluate(X, Y, w)\n",
    "\n",
    "print(\"Out-of-sample Evaluation:\")\n",
    "evaluate(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9423b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449875a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In-sample Evaluation:\")\n",
    "evaluate(X, Y, w)\n",
    "\n",
    "print(\"Out-of-sample Evaluation:\")\n",
    "evaluate(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the label distributions\n",
    "print(np.bincount(np.round(Y_test).ravel().astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model coefficients interpretation:\")\n",
    "for (coef, name) in zip([b[0], *list(w)], \n",
    "                    [\"Bias\", \"Num Qubits\", \"Circuit Depth\", \"ECR Count\", \"RZ Count\", \"SX Count\", \"X Count\"]):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdf695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedledum-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
